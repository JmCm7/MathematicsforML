{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Assigment 1\n",
        "\n",
        "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
        "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
        "\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
        "\n",
        "You should:\n",
        "\n",
        "1) Specify which Machine Learning problem are you solving.\n",
        "\n",
        "2) Provide a short summary of the features and the labels you are working on.\n",
        "\n",
        "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
        "\n",
        "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
        "\n",
        "5) Show some examples to illustrate that the method is working properly.\n",
        "\n",
        "6) Provide quantitative evidence for generalization using the provided dataset.\n"
      ],
      "metadata": {
        "id": "s-y8Kil2snGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5AOO-Ib6o_7U",
        "outputId": "7b41bb53-dc93-47cc-9961-556056388581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "Optimized a step.\n",
            "Optimized a step.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        opt_dict = {}\n",
        "\n",
        "        transforms = [[1,1],\n",
        "                      [-1,1],\n",
        "                      [-1,-1],\n",
        "                      [1,-1]]\n",
        "\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        print(self.w)\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "        \n",
        "        \n",
        "data_dict = {-1:np.array([[1,7],\n",
        "                          [2,8],\n",
        "                          [3,8],]),\n",
        "             \n",
        "             1:np.array([[5,1],\n",
        "                         [6,-1],\n",
        "                         [7,3],])}\n",
        "\n",
        "svm1 = Support_Vector_Machine()\n",
        "svm1.fit(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm1.predict([7,3.5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmE-qAQOqwJ8",
        "outputId": "39954890-eead-42db-f679-f7af734b76f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.224 -0.224]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Optimization\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Machine Learning Models and Metrics.\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import svm\n",
        "\n",
        "# Visualization libraries.\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "import plotly.express as px\n",
        "from plotly.offline import plot\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "3gy0JoXQAKrX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SOLUTION** \n",
        "\n",
        "**Introduction:**\n",
        "In this assignment, we are gonna try to solve a Machin Learning problem using SVM, we will study the features and labels of the information we are working with, check that the datasets and their properties are ideal for our model, explain the code that it is giving to us and addapt it to our problem, give some examples to verify the proper working of the code and finally test how good was the generalization of our model.\n",
        "\n",
        "\n",
        "\n",
        "1)Both problems are about classify new data using the information we already have, which have been classified. In the first problem, we want to use our model in banknote authentication and with the second one, we look to know about he occupation or not of a room. in both cases we have to choose between two options to classify. Then we are in front of a supervised binary classification problem.\n",
        "\n",
        "__________________________________________________________________________________\n",
        "\n",
        "2) Summary of the features and the labels:\n",
        "\n",
        "\n",
        "1.   First problem (Banknote Authentication):\n",
        "\n",
        "\n",
        "*   Features: We have the next four features:\n",
        "\n",
        "\n",
        "  a). Variance of Wavelet Transformed image (continuous)\n",
        "\n",
        "\n",
        "  b). Skewness of Wavelet Transformed image (continuous) \n",
        "\n",
        "  c). Curtosis of Wavelet Transformed image (continuous) \n",
        "\n",
        "  d). Entropy of image (continuous) \n",
        "\n",
        "  These features were extracted using the Wavelet Transform, which is a mathematical tool developed to analyze data where features vary over different scales. As we can look in the features, these take real values in a continous way.\n",
        "\n",
        "\n",
        "*  Labels: We only have two labels 0 and 1. We don't have the information about the meaning of the labels, but for the purpose of this proyect we will understand the label 1 as \"Genuine\" and the label 0 as \"Forgery\".\n",
        "\n",
        "\n",
        "\n",
        "2.   Second Problem (Occupancy Detection):\n",
        "\n",
        "\n",
        "*   Features: For the second probem we have seven features, these are:\n",
        "\n",
        "a). Date time year-month-day.\n",
        "\n",
        "b). Hour:minute:second.\n",
        "\n",
        "c). Temperature, in Celsius.\n",
        "\n",
        "d). Relative Humidity, % \n",
        "\n",
        "e). Light, in Lux \n",
        "\n",
        "f). CO2, in ppm \n",
        "\n",
        "g). Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air \n",
        "\n",
        "\n",
        "*   Labels: Here we have again to labels: 0 and 1. In this problem we understand 0 as \"not occupied\" and 1 as\"occupied\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mvymtgUyQCMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) \n",
        "a. We will study both datasets using the perceptron method. This method allow us to check if the datasets are linearly separable because it is a linear classifier. Therefore if we use the method over all the training data and then we test its accuracy against the training data again, we will get an accuracy of 100 per cent only if the data set is linearly separable. Following this idea we import the perceptron method from sklearn and implement the next code:"
      ],
      "metadata": {
        "id": "wzUF7_ikDvGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", \n",
        "                 sep = ',', \n",
        "                 header = None, \n",
        "                 names=[\"variance_of_Wavelet\",\"skewness_of_Wavelet\",\n",
        "                        \"curtosis_of_Wavelet\",\"entropy\",\n",
        "                        \"class\"],\n",
        "                 thousands = ',')\n",
        "variables=[\"date\",\"temperature\", \"curtosis_of_Wavelet\",\"entropy\"]\n",
        "\n",
        "\n",
        "train_set_df, test_set_df= train_test_split(df, test_size=0.2,random_state=42)\n",
        "\n",
        "#Definition of the features X and the labels y for training\n",
        "X=train_set_df[['variance_of_Wavelet', 'skewness_of_Wavelet', 'curtosis_of_Wavelet','entropy']]\n",
        "y=train_set_df[\"class\"]\n",
        "#df.keys()\n",
        "\n",
        " #Definition of the features X_test and the labels y_test for testing\n",
        "X_test=test_set_df[['variance_of_Wavelet', 'skewness_of_Wavelet', 'curtosis_of_Wavelet','entropy']]\n",
        "y_test=test_set_df['class']\n",
        "\n"
      ],
      "metadata": {
        "id": "hFWb99aaahMT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import svm\n",
        "\n",
        "\n",
        "clf_p = Perceptron(tol=1e-3, random_state=0)\n",
        "clf_p.fit(X, y)\n",
        "print('Accuracy training set:')\n",
        "\n",
        "\n",
        "clf_p.score(X, y)\n",
        "\n",
        "\n",
        "predicted_perceptron_test=clf_p.predict(X_test)\n",
        "\n",
        "#Accuracy measure for the testing dataset\n",
        "\n",
        "accuracy_test= accuracy_score(y_test, predicted_perceptron_test).round(2)\n",
        "conf_matrix_test= confusion_matrix(y_test, predicted_perceptron_test)\n",
        "\n",
        "print('Accuracy testing set:',accuracy_test)\n",
        "print('Confusion matrix:\\n', conf_matrix_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O5ZNxegXaMC",
        "outputId": "988ff31b-f89a-4772-ad09-710fccb41bd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy training set:\n",
            "Accuracy testing set: 0.95\n",
            "Confusion matrix:\n",
            " [[145   3]\n",
            " [ 12 115]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first block of code we observe that the accuracy is 95 per cent, so we can conclude that the dataset for the first problem isn't linearly separable. Now for the second problem we have to save the data and also transform the date to a number we can work with:\n"
      ],
      "metadata": {
        "id": "u2FAI_0CuBvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def load_occupancy_data():\n",
        "    tarball_path = Path(\"datasets/occupancy_data.zip\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with zipfile.ZipFile(tarball_path) as occupancy_tarball:\n",
        "           # open the csv file in the dataset\n",
        "           occupancy_tarball.extractall(path=\"datasets\")\n",
        "    list_df =[pd.read_csv(Path(\"datasets/datatraining.txt\"),parse_dates=['date'],date_parser=dateparse),\n",
        "              pd.read_csv(Path(\"datasets/datatest.txt\"),parse_dates=['date'],date_parser=dateparse),\n",
        "              pd.read_csv(Path(\"datasets/datatest2.txt\"),parse_dates=['date'],date_parser=dateparse),]\n",
        "    return list_df\n",
        "\n",
        "train, test1, test2= load_occupancy_data()\n",
        "\n",
        "train['date_numeric'] = train['date'].apply(lambda time: time.year+time.month/12+ time.day/365 + time.hour/8760+time.minute/525600)\n",
        "test1['date_numeric'] = test1['date'].apply(lambda time: time.year+time.month/12+ time.day/365 + time.hour/8760+time.minute/525600)\n",
        "test2['date_numeric'] = test2['date'].apply(lambda time: time.year+time.month/12+ time.day/365 + time.hour/8760+time.minute/525600)"
      ],
      "metadata": {
        "id": "N57akVbwkKul"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will only observe if the train data and test 1 is separable or not, after all, we should keep a set of test data to work after all."
      ],
      "metadata": {
        "id": "G5NB48qolJnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_o=train[['date_numeric', 'Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']]\n",
        "y_train_o=train['Occupancy']\n",
        "X_test1_o=test1[['date_numeric', 'Temperature', 'Humidity', 'Light', \n",
        "              'CO2', 'HumidityRatio']]\n",
        "y_test1_o=test1['Occupancy']\n",
        "clf_p = Perceptron(tol=1e-3, random_state=0)\n",
        "clf_p.fit(X_train_o, y_train_o)\n",
        "print('Accuracy training set:')\n",
        "clf_p.score(X_train_o, y_train_o)\n",
        "\n",
        "predicted_perceptron_test1_o=clf_p.predict(X_test1_o)\n",
        "print('Accuracy testing set 1:')\n",
        "clf_p.score(X_test1_o, y_test1_o)\n",
        "\n",
        "conf_matrix_test1_o= confusion_matrix(y_test1_o, predicted_perceptron_test1_o)\n",
        "\n",
        "accuracy_test= accuracy_score(y_test1_o, predicted_perceptron_test1_o).round(2)\n",
        "#print('Acuracy:',accuracy_test1_o)\n",
        "print('Accuracy testing set:',accuracy_test)\n",
        "print('Confusion matrix:\\n', conf_matrix_test1_o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhvDYdHclDtR",
        "outputId": "00b9b312-d98b-4906-ab3d-0a1d010af0b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy training set:\n",
            "Accuracy testing set 1:\n",
            "Accuracy testing set: 0.95\n",
            "Confusion matrix:\n",
            " [[1648   45]\n",
            " [  79  893]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that we again get a value different from 100% accuracy, so we can conclude that the dataset from the second problem isn't linearly separable."
      ],
      "metadata": {
        "id": "UaTJWW_el4db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. b) The datasets are not randomly chosen because the data was selected using an especific method. In the first datesets, the vectors are created selecting the specimens as genuine and forged banknote-like objetcs, with a formar of 400 x 400 pixels and using a Wavelet transform to extract the features we observe.\n",
        "\n",
        "In the second case, we have the same problem, the data are not rndomly chosen, because we are testing the occupation of an specific room. There is certain level of control over the variables of the experiment from which, we are getting the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "DlWdhKjKB5Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. c) We can never guarantee generalization, the only thing we can guarantee when the dataset is big enough, is a high propability that the error produced by our hypothesis is small. After all we don't known which is the generalization function we are aiming for."
      ],
      "metadata": {
        "id": "wMybr6-0uXTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4). Using the first code example given at the begining, I have modified the code to get the next algorithm for my SVM."
      ],
      "metadata": {
        "id": "gMqUhcizvfyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ZvmgQco3he"
      },
      "outputs": [],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "#First we import the libraries which the tools and functions we will need.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "style.use('ggplot')\n",
        "\n",
        "#Here we will create our SVM\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=False):\n",
        "      self.visualization = visualization\n",
        "      self.colors = {1:'r',-1:'b'}\n",
        "      if self.visualization:\n",
        "          self.fig = plt.figure()\n",
        "          self.ax = self.fig.add_subplot(1,1,1)\n",
        "#I have removed the visualization parameter because we are going to work with vectors in R^4\n",
        "    # train\n",
        "    #In fit the last parameter t, is a new parameter that we use to limit the number of hyperplanes we will check. If t is small we will check \n",
        "    #more hyperplanes and the code will run slower \n",
        "    def fit(self, data, t=45):\n",
        "      self.data = data\n",
        "      # { ||w||: [w,b] }\n",
        "      opt_dict = {}\n",
        "\n",
        "      #transforms: We will use theses to change w and check different possibilities for the Hyperplane, here we use t.\n",
        "      r= (t*np.pi)/180\n",
        "      transforms = [] #In this list we will place all the transformation that we will use\n",
        "      for angulo1 in np.arange(0 , 2*np.pi , r):\n",
        "        for angulo2 in np.arange(0 , 2*np.pi , r):\n",
        "          for angulo3 in np.arange(0 , 2*np.pi , r):\n",
        "            transforms.append([np.cos(angulo1),np.sin(angulo1)*np.cos(angulo2),np.sin(angulo1)*np.sin(angulo2)*np.cos(angulo3),\n",
        "                            np.sin(angulo1)*np.sin(angulo2)*np.sin(angulo3)]) #We are using different points in a 4 dimensional sphere to define the transformations\n",
        "\n",
        "\n",
        "#Here we list all the data in a single list without distingish among features \n",
        "      all_data = []\n",
        "      for yi in self.data:\n",
        "          for featureset in self.data[yi]:\n",
        "              for feature in featureset:\n",
        "                  all_data.append(feature)\n",
        "#We take the maximun and minimun value among the values of all the features\n",
        "      self.max_feature_value = max(all_data)\n",
        "      self.min_feature_value = min(all_data)\n",
        "      all_data = None\n",
        "\n",
        "      # support vectors yi(xi.w+b) = 1\n",
        "      \n",
        "#We define the size of the sets we are going to use in each iteration\n",
        "      step_sizes = [self.max_feature_value * 0.1,\n",
        "                    self.max_feature_value * 0.01,\n",
        "                    # point of expense:\n",
        "                    self.max_feature_value * 0.001,]\n",
        "\n",
        "      \n",
        "      \n",
        "      # extremely expensive\n",
        "      b_range_multiple = 5\n",
        "      # we dont need to take as small of steps\n",
        "      # with b as we do w\n",
        "      b_multiple = 5\n",
        "      latest_optimum = self.max_feature_value*10\n",
        "\n",
        "      #Here we begin the process to get good values for b and w\n",
        "      for step in step_sizes:\n",
        "          #We begin with an arbitrary w using the greatest value aomng the features\n",
        "          w = np.array([latest_optimum,latest_optimum,latest_optimum,latest_optimum])\n",
        "          # we can do this because convex\n",
        "          optimized = False\n",
        "          #The while repeats itself until it has tested all possible configuration\n",
        "          while not optimized:\n",
        "            \n",
        "              for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                  self.max_feature_value*b_range_multiple,\n",
        "                                  step*b_multiple):\n",
        "                  for transformation in transforms:\n",
        "                      w_t = w*transformation\n",
        "                      found_option = True\n",
        "                      # weakest link in the SVM fundamentally\n",
        "                      # SMO attempts to fix this a bit\n",
        "                      # yi(xi.w+b) >= 1\n",
        "                      # \n",
        "                      # #### add a break here later..\n",
        "                      for i in self.data:\n",
        "                          for xi in self.data[i]:\n",
        "                              yi=i\n",
        "                              # Verify constraints\n",
        "                              if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                  found_option = False\n",
        "                                  \n",
        "                      if found_option:\n",
        "                          # Computes norm\n",
        "                          opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "              #Here we test that we have proved all values of w\n",
        "              if w[0] < 0:\n",
        "                  optimized = True\n",
        "                  print('Optimized a step.')\n",
        "              else:\n",
        "                  w = w - step\n",
        "\n",
        "          norms = sorted([n for n in opt_dict])\n",
        "          #||w|| : [w,b]\n",
        "          opt_choice = opt_dict[norms[0]]\n",
        "          self.w = opt_choice[0]\n",
        "          self.b = opt_choice[1]\n",
        "          latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        if classification !=0 and self.visualization:\n",
        "            self.ax.scatter(features[0], features[1], s=200, marker='*', c=self.colors[classification]) #This is useful to visualize when we are working in spaces of diemnsion 2\n",
        "        return classification\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Then we use the next little set of data to test that the algorithm is working in the hoped way:"
      ],
      "metadata": {
        "id": "D4NQotJ2G2Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {-1:np.array([[1,2,3,4],\n",
        "                          [2,3,4,5],\n",
        "                          [3,4,5,6]]),\n",
        "             \n",
        "             1:np.array([[-1,-2,-3,-4],\n",
        "                          [-2,-3,-4,-5],\n",
        "                          [-3,-4,-5,-6]])}\n",
        "\n",
        "svm = Support_Vector_Machine()\n",
        "svm.fit(data_dict,45)\n",
        "\n",
        "predict_us = [[4,5,6,7],[-4,-5,-6,-7],[0,0,0,1]]\n",
        "\n",
        "for p in predict_us:\n",
        "    print(svm.predict(p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J92RTh4tyrTx",
        "outputId": "c0e9de80-6530-4462-aa4b-1a6498070ca0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "Optimized a step.\n",
            "Optimized a step.\n",
            "-1.0\n",
            "1.0\n",
            "-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. To show cuantitative evidence of generalization we have to split the data set in two: a train dataset and a test dataset. We have already done that in our verification that the data was linearly separable.\n",
        "\n",
        "However the code we have created takes to long and it isn't optimized, so we will left the last point without solution with our in this case and we will look the output of a SVM from Sklearn."
      ],
      "metadata": {
        "id": "DaqRItnDUkfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf= svm.SVC()\n",
        "clf.fit(X, y)\n",
        "\n",
        "predicted_svm= clf.predict(X)\n",
        "predicted_svm_test=clf.predict(X_test)\n",
        "\n",
        "accuracy_train= accuracy_score(y, predicted_svm).round(2)\n",
        "conf_matrix_train= confusion_matrix(y , predicted_svm)\n",
        "\n",
        "accuracy_test= accuracy_score(y_test, predicted_svm_test).round(2)\n",
        "conf_matrix_test= confusion_matrix(y_test, predicted_svm_test)\n",
        "\n",
        "print('Accuracy:',accuracy_test)\n",
        "print('Confusion matrix:\\n', conf_matrix_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TJ1emP4c5Wp",
        "outputId": "823841bf-7bba-4cd2-d9d8-260b8d2e408d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion matrix:\n",
            " [[148   0]\n",
            " [  0 127]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2_o=test2[['date_numeric', 'Temperature', 'Humidity', 'Light', \n",
        "              'CO2', 'HumidityRatio']]\n",
        "y_test2_o=test2['Occupancy']\n",
        "\n",
        "clf= svm.SVC()\n",
        "clf.fit(X_train_o, y_train_o)\n",
        "\n",
        "predicted_svm= clf.predict(X_train_o)\n",
        "predicted_svm_test=clf.predict(X_test2_o)\n",
        "\n",
        "accuracy_train= accuracy_score(y_train_o, predicted_svm).round(2)\n",
        "conf_matrix_train= confusion_matrix(y_train_o, predicted_svm)\n",
        "\n",
        "accuracy_test= accuracy_score(y_test2_o, predicted_svm_test).round(2)\n",
        "conf_matrix_test= confusion_matrix(y_test2_o, predicted_svm_test)\n",
        "\n",
        "print('Accuracy:',accuracy_test)\n",
        "print('Confusion matrix:\\n', conf_matrix_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2e_8vYGm36u",
        "outputId": "74810aec-d9a4-4f48-b409-1e979f0a540f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.99\n",
            "Confusion matrix:\n",
            " [[7566  137]\n",
            " [   8 2041]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see something really interesting here.In the point 3 we use the perceptron method to prove if the dataset was linealy separable or not, and we conclude that it wasn't. However at the end using SVM, we get an accuracy of 100%,i the first problem. We can get that only if the dataset is linearly separable. Then our observation with the perceptron was wrong. On the other side the accuracy in the second problom was also higher than the expected."
      ],
      "metadata": {
        "id": "qIb5DbjqdOR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias: \n",
        "\n",
        "\n",
        "*   https://www.mathworks.com/discovery/wavelet-transforms.html\n",
        "\n"
      ],
      "metadata": {
        "id": "nE_gus48Eodb"
      }
    }
  ]
}